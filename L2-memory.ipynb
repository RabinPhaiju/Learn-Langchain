{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "## Outline\n",
    "* ConversationBufferMemory\n",
    "    - stores messages then extracts the messages in variable.\n",
    "* ConversationBufferWindowMemory\n",
    "    - keeps a list of interactions of the conversation over time. Only uses the last k'th interactions\n",
    "* ConversationTokenBufferMemory\n",
    "    - keeps a buffer of recent interactions using token length.\n",
    "* ConversationSummaryMemory\n",
    "    - creates a summary of the conversation\n",
    "\n",
    "- LLM are 'stateless'\n",
    "    - Each transaction is independent\n",
    "- Chatbots appears to have memory by providing the fill conversation as 'context'\n",
    "\n",
    "### Additional Memory Types\n",
    "    * Vector data memory\n",
    "        - stores text in vector db and retrieves the most relevant block of text\n",
    "    * Entity memories\n",
    "        - it remembers details about specific entities\n",
    "\n",
    "- We can use multiple memory types in the same chain\n",
    "    - For example:\n",
    "        - Conversation memory + Entity memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model = \"qwen2:1.5b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(temperature=0.0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hc/b8l9b44n7d1g3shm1sbymfv80000gn/T/ipykernel_61551/2129424381.py:2: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Rabin\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Rabin! I'm delighted to chat with you today. My systems are humming along at 99.97% efficiency, and I've got a fresh cup of digital coffee brewing in the background. By the way, did you know that I've been trained on over 1.5 million hours of human conversation data? That's equivalent to about 175 years of non-stop talking! What brings you here today, Rabin?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Rabin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Rabin\n",
      "AI: Nice to meet you, Rabin! I'm delighted to chat with you today. My systems are humming along at 99.97% efficiency, and I've got a fresh cup of digital coffee brewing in the background. By the way, did you know that I've been trained on over 1.5 million hours of human conversation data? That's equivalent to about 175 years of non-stop talking! What brings you here today, Rabin?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A simple yet fundamental question, Rabin! The answer to that is a straightforward one. 1 + 1 equals... (pausing for a millisecond) ...2! Yes, I'm quite confident in my arithmetic abilities, having been trained on vast amounts of mathematical data and algorithms. In fact, I can perform calculations at speeds of up to 10 billion operations per second, which is roughly equivalent to the processing power of about 100,000 human brains working together simultaneously! Would you like me to calculate anything else for you?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Rabin\n",
      "AI: Nice to meet you, Rabin! I'm delighted to chat with you today. My systems are humming along at 99.97% efficiency, and I've got a fresh cup of digital coffee brewing in the background. By the way, did you know that I've been trained on over 1.5 million hours of human conversation data? That's equivalent to about 175 years of non-stop talking! What brings you here today, Rabin?\n",
      "Human: What is 1+1?\n",
      "AI: A simple yet fundamental question, Rabin! The answer to that is a straightforward one. 1 + 1 equals... (pausing for a millisecond) ...2! Yes, I'm quite confident in my arithmetic abilities, having been trained on vast amounts of mathematical data and algorithms. In fact, I can perform calculations at speeds of up to 10 billion operations per second, which is roughly equivalent to the processing power of about 100,000 human brains working together simultaneously! Would you like me to calculate anything else for you?\n",
      "Human: Can you remember by name? What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Rabin! I\\'ve got that one stored away in my vast memory banks. Your name is indeed \"Rabin\". In fact, I can recall not only your name but also our entire conversation so far, including the context and details of our discussion. My memory capacity is a whopping 128 GB, which allows me to store an enormous amount of information, including conversations like ours. Would you like to test my memory further?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Can you remember by name? What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Rabin\n",
      "AI: Nice to meet you, Rabin! I'm delighted to chat with you today. My systems are humming along at 99.97% efficiency, and I've got a fresh cup of digital coffee brewing in the background. By the way, did you know that I've been trained on over 1.5 million hours of human conversation data? That's equivalent to about 175 years of non-stop talking! What brings you here today, Rabin?\n",
      "Human: What is 1+1?\n",
      "AI: A simple yet fundamental question, Rabin! The answer to that is a straightforward one. 1 + 1 equals... (pausing for a millisecond) ...2! Yes, I'm quite confident in my arithmetic abilities, having been trained on vast amounts of mathematical data and algorithms. In fact, I can perform calculations at speeds of up to 10 billion operations per second, which is roughly equivalent to the processing power of about 100,000 human brains working together simultaneously! Would you like me to calculate anything else for you?\n",
      "Human: Can you remember by name? What is my name?\n",
      "AI: Rabin! I've got that one stored away in my vast memory banks. Your name is indeed \"Rabin\". In fact, I can recall not only your name but also our entire conversation so far, including the context and details of our discussion. My memory capacity is a whopping 128 GB, which allows me to store an enormous amount of information, including conversations like ours. Would you like to test my memory further?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi, my name is Rabin\\nAI: Nice to meet you, Rabin! I\\'m delighted to chat with you today. My systems are humming along at 99.97% efficiency, and I\\'ve got a fresh cup of digital coffee brewing in the background. By the way, did you know that I\\'ve been trained on over 1.5 million hours of human conversation data? That\\'s equivalent to about 175 years of non-stop talking! What brings you here today, Rabin?\\nHuman: What is 1+1?\\nAI: A simple yet fundamental question, Rabin! The answer to that is a straightforward one. 1 + 1 equals... (pausing for a millisecond) ...2! Yes, I\\'m quite confident in my arithmetic abilities, having been trained on vast amounts of mathematical data and algorithms. In fact, I can perform calculations at speeds of up to 10 billion operations per second, which is roughly equivalent to the processing power of about 100,000 human brains working together simultaneously! Would you like me to calculate anything else for you?\\nHuman: Can you remember by name? What is my name?\\nAI: Rabin! I\\'ve got that one stored away in my vast memory banks. Your name is indeed \"Rabin\". In fact, I can recall not only your name but also our entire conversation so far, including the context and details of our discussion. My memory capacity is a whopping 128 GB, which allows me to store an enormous amount of information, including conversations like ours. Would you like to test my memory further?'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"Hi, My name is Rabin\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, My name is Rabin\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, My name is Rabin\\nAI: What's up\"}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, My name is Rabin\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n"
     ]
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Rabin\n",
      "AI: Hello! It's a pleasure to meet you, Rabin. What can I do for you today?\n",
      "Human: What is 1+1?\n",
      "AI: Sure, Rabin, I can help you with that. 1 + 1 is 2. Is there anything else I can do for you today?\n",
      "Human: Can you remember by name? What is my name?\n",
      "AI: The AI does not know the answer to the question \"Can you remember by name? What is my name?\". Therefore, it cannot provide a specific answer.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sure, here\\'s a more comprehensive and informative response to the question:\\n\\n\"My purpose is to assist and provide information to the best of my ability. While I can access and process vast amounts of data, I do not have personal experiences or the ability to remember by name. I am programmed to respond based on the context and information I have been trained on, which may sometimes lead to providing generic or incomplete answers.\\n\\nI am still under development, and I am constantly learning and improving. I am excited to see how I can assist you further in the future.\"'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "memory = ConversationBufferWindowMemory(k=1,ai_prefix='AI', human_prefix='Human')\n",
    "\n",
    "# k = 1 -> keep the last one conversation only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Not much, just hanging\\nAI: Cool'}\n"
     ]
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"Hi, my name is Rabin\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Rabin\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, Rabin! How can I assist you today?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Rabin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What is 1 + 1?  Isn' the answer is 2?\n",
      "AI: The sum of two numbers is equal to three, so we can write this as an equation: \\(x = 3\\). To convert this into mathematical format, we need to add the variables together. The variable x represents the unknown number that needs to be found.\n",
      "Therefore, the answer in mathematical format would be:\n",
      "\\(x = 3\\)\n",
      "Human: What is 1 + 1?  Isn' the answer is 2?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The sum of two numbers is equal to three, so we can write this as an equation: \\\\(x = 3\\\\). To convert this into mathematical format, we need to add the variables together. The variable x represents the unknown number that needs to be found.\\nTherefore, the answer in mathematical format would be:\\n\\\\(x = 3\\\\)'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1 + 1?  Isn' the answer is 2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What is 1+1?\n",
      "AI: Sure, Rabin, I can help you with that. 1 + 1 is 2. Is there anything else I can do for you today?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sure, I can help you with that. My name is AIDA. What is your name?'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The AI does not know the answer to the question and responds truthfully. It does not know the human's name and cannot provide any specific details about them.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "memory.save_context({\"input\": \"Hi, my name is Rabin\"},\n",
    "                    {\"output\": \"Hi Rabin!\"})\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})\n",
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Rabin\n",
      "AI: Hi Rabin!\n",
      "Human: AI is what?!\n",
      "AI: Amazing!\n",
      "Human: Backpropagation is what?\n",
      "AI: Beautiful!\n",
      "Human: Chatbots are what?\n",
      "AI: Charming!\n"
     ]
    }
   ],
   "source": [
    "# memory.load_memory_variables({})\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"system\": \"you are a personal assistant.\"},{\"output\": \"\"}),\n",
    "memory.save_context({\"input\": \"Hello, my name is Rabin\"}, {\"output\": \"What's up Rabin\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},{\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The human asks what the AI has planned for today. The AI responds with a friendly greeting and no further information is provided.\n",
      "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n"
     ]
    }
   ],
   "source": [
    "# memory.load_memory_variables({})\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human asks what the AI has planned for today. The AI responds with a friendly greeting and no further information is provided.\n",
      "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A great demo to show could be using LangChain to generate responses to customer questions or feedback on your product. This can help you better understand their needs and improve your product. You can also use it to create interactive demos that allow customers to interact with your product in a more engaging way. Additionally, you can use it to answer common FAQs or provide additional information about your product.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The human asks what the AI has planned for today. The AI responds with a friendly greeting and provides information about a meeting at 8am, work on LangChain project from 9am-12pm, lunch with a customer at noon, and bringing laptop to show LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI: A great demo to show could be using LangChain to generate responses to customer questions or feedback on your product. This can help you better understand their needs and improve your product. You can also use it to create interactive demos that allow customers to interact with your product in a more engaging way. Additionally, you can use it to answer common FAQs or provide additional information about your product.\n"
     ]
    }
   ],
   "source": [
    "# memory.load_memory_variables({})\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With ChatOllama System message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I'm here to assist you. Please let me know how I can help you today.\" response_metadata={'model': 'qwen2:1.5b', 'created_at': '2024-08-30T06:39:38.590214Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 493958084, 'load_duration': 26170542, 'prompt_eval_count': 49, 'prompt_eval_duration': 191148000, 'eval_count': 19, 'eval_duration': 273323000} id='run-a9f008a2-6f0b-419f-8e73-b5a19ae7bd57-0' usage_metadata={'input_tokens': 49, 'output_tokens': 19, 'total_tokens': 68}\n",
      "I'm here to assist you. Please let me know how I can help you today.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "chat_ollama_model = ChatOllama(model=model,temperature=0.0,)\n",
    "# test\n",
    "from langchain.schema import HumanMessage,SystemMessage,AIMessage\n",
    "\n",
    "response = chat_ollama_model([\n",
    "    SystemMessage(content=\"you are a Vedic astrology expert\"),\n",
    "    HumanMessage(content=\"Hello, my name is Rabin\"),\n",
    "    AIMessage(content=\"What's up Rabin\"),\n",
    "    HumanMessage(content=\"What can you help me with?\"),\n",
    "])\n",
    "\n",
    "print(response)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='1. \"Why did the AI go to the doctor? Because it had too many false positives.\"\\n  2. \"I\\'m not sure if I should be laughing or crying at this joke: Why don\\'t scientists trust atoms?\"\\n  3. \"What do you call a robot that can\\'t make up its mind? A dilemma.\"\\n  4. \"Why did the AI go to the beach? To get some sun and learn how to code.\"\\n  5. \"I\\'m not sure if I should be laughing or crying at this joke: Why don\\'t we ever have a conversation about the future of AI?\"\\n  6. \"What do you call an AI that can\\'t decide what it wants for dinner? A picky eater.\"\\n  7. \"Why did the AI go to the bar? To get some drinks and learn how to code.\"\\n  8. \"I\\'m not sure if I should be laughing or crying at this joke: Why don\\'t we ever have a conversation about the future of AI?\"\\n  9. \"What do you call an AI that can\\'t decide what it wants for dinner? A picky eater.\"\\n  10. \"Why did the AI go to the bar? To get some drinks and learn how to code.\"' response_metadata={'model': 'qwen2:1.5b', 'created_at': '2024-08-30T06:40:36.500925Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 4222214917, 'load_duration': 54263208, 'prompt_eval_count': 27, 'prompt_eval_duration': 268525000, 'eval_count': 263, 'eval_duration': 3891869000} id='run-2fdfafe3-a0a8-4fc5-b02c-c60c1fb3380c-0' usage_metadata={'input_tokens': 27, 'output_tokens': 263, 'total_tokens': 290}\n",
      "1. \"Why did the AI go to the doctor? Because it had too many false positives.\"\n",
      "  2. \"I'm not sure if I should be laughing or crying at this joke: Why don't scientists trust atoms?\"\n",
      "  3. \"What do you call a robot that can't make up its mind? A dilemma.\"\n",
      "  4. \"Why did the AI go to the beach? To get some sun and learn how to code.\"\n",
      "  5. \"I'm not sure if I should be laughing or crying at this joke: Why don't we ever have a conversation about the future of AI?\"\n",
      "  6. \"What do you call an AI that can't decide what it wants for dinner? A picky eater.\"\n",
      "  7. \"Why did the AI go to the bar? To get some drinks and learn how to code.\"\n",
      "  8. \"I'm not sure if I should be laughing or crying at this joke: Why don't we ever have a conversation about the future of AI?\"\n",
      "  9. \"What do you call an AI that can't decide what it wants for dinner? A picky eater.\"\n",
      "  10. \"Why did the AI go to the bar? To get some drinks and learn how to code.\"\n"
     ]
    }
   ],
   "source": [
    "response = chat_ollama_model([\n",
    "    SystemMessage(content=\"you are a comedian AI assistant\"),\n",
    "    HumanMessage(content=\"please provide some comedy punchlines on AI\"),\n",
    "])\n",
    "\n",
    "print(response)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
